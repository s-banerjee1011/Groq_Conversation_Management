{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a9c95e",
   "metadata": {},
   "source": [
    "# Conversation Management & Classification using Groq API\n",
    "\n",
    "This Colab-ready notebook implements:\n",
    "- Task 1: Conversation history manager with truncation and periodic summarization (LLM + fallback).\n",
    "- Task 2: JSON Schema extraction using OpenAI-compatible function calling (Groq endpoint) with robust parsing and offline regex fallback.\n",
    "\n",
    "**Security note:** Do NOT commit API keys to GitHub or paste them in public chat. If you have exposed a key, revoke it in the Groq console and create a new one before using this notebook.\n",
    "\n",
    "Run cells top-to-bottom. Enter your Groq API key in the secure prompt cell (hidden input) if you want LLM-powered behavior; otherwise offline fallbacks will run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --quiet openai jsonschema requests\n",
    "print('Installed dependencies: openai, jsonschema, requests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secure prompt for GROQ API key (hidden)\n",
    "from getpass import getpass\n",
    "import os, textwrap\n",
    "print(\"IMPORTANT: If you exposed an API key publicly earlier, revoke it in the Groq console BEFORE continuing.\")\n",
    "GROQ_API_KEY = getpass('Paste your GROQ API key here (hidden). Leave empty to use offline fallbacks: ')\n",
    "if GROQ_API_KEY:\n",
    "    os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "    print('GROQ_API_KEY set for this session.')\n",
    "else:\n",
    "    print(textwrap.dedent('''\n",
    "    No key provided - LLM calls will be disabled and offline fallbacks will be used.\n",
    "    To enable LLM calls, rerun this cell and paste a valid Groq API key (do not paste keys in public chat).\n",
    "    ''').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd034f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI-compatible client for Groq and auto-select model (if key provided)\n",
    "import os, requests, json, time\n",
    "use_llm = False\n",
    "client = None\n",
    "MODEL = None\n",
    "\n",
    "API_KEY = os.environ.get('GROQ_API_KEY')\n",
    "if API_KEY:\n",
    "    try:\n",
    "        import openai\n",
    "        try:\n",
    "            client = openai.OpenAI(api_key=API_KEY, base_url='https://api.groq.com/openai/v1')\n",
    "            use_llm = True\n",
    "            print('Configured openai.OpenAI(...) client for Groq.')\n",
    "        except Exception:\n",
    "            openai.api_key = API_KEY\n",
    "            openai.api_base = 'https://api.groq.com/openai/v1'\n",
    "            client = openai\n",
    "            use_llm = True\n",
    "            print('Configured openai module-level client for Groq (fallback).')\n",
    "    except Exception as e:\n",
    "        print('Failed to import openai library:', e)\n",
    "        use_llm = False\n",
    "else:\n",
    "    print('No GROQ_API_KEY in environment; LLM disabled. Use offline fallbacks.')\n",
    "\n",
    "print('use_llm =', use_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model discovery & probe utilities\n",
    "import requests, json, time\n",
    "\n",
    "def list_models_from_groq(api_key: str):\n",
    "    try:\n",
    "        url = 'https://api.groq.com/openai/v1/models'\n",
    "        headers = {'Authorization': f'Bearer {api_key}'}\n",
    "        r = requests.get(url, headers=headers, timeout=20)\n",
    "        if r.status_code != 200:\n",
    "            print('Models list request failed:', r.status_code, r.text[:500])\n",
    "            return []\n",
    "        data = r.json()\n",
    "        models = data.get('data') or data.get('models') or data\n",
    "        model_ids = []\n",
    "        if isinstance(models, list):\n",
    "            for m in models:\n",
    "                if isinstance(m, dict):\n",
    "                    mid = m.get('id') or m.get('model') or None\n",
    "                else:\n",
    "                    mid = str(m)\n",
    "                if mid:\n",
    "                    model_ids.append(mid)\n",
    "        return model_ids\n",
    "    except Exception as e:\n",
    "        print('list_models_from_groq error:', e)\n",
    "        return []\n",
    "\n",
    "def probe_model_candidate(api_key: str, model_name: str, client_obj=None, timeout=10):\n",
    "    try:\n",
    "        if client_obj is not None:\n",
    "            try:\n",
    "                resp = client_obj.chat.completions.create(model=model_name, messages=[{'role':'user','content':'Hi'}], max_tokens=1, temperature=0)\n",
    "                return True, resp\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    resp2 = client_obj.ChatCompletion.create(model=model_name, messages=[{'role':'user','content':'Hi'}], max_tokens=1, temperature=0)\n",
    "                    return True, resp2\n",
    "                except Exception as e2:\n",
    "                    return False, f'client errors: {e1} | {e2}'\n",
    "        url = 'https://api.groq.com/openai/v1/chat/completions'\n",
    "        headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "        payload = {'model': model_name, 'messages': [{'role':'user','content':'Hi'}], 'max_tokens':1}\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return True, r.json()\n",
    "        else:\n",
    "            return False, f'http {r.status_code}: {r.text[:800]}'\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def auto_select_model(api_key: str, client_obj=None):\n",
    "    if not api_key:\n",
    "        return None\n",
    "    print('Listing models from Groq...')\n",
    "    available = list_models_from_groq(api_key)\n",
    "    print('Models reported by Groq:', available[:50])\n",
    "    candidates = []\n",
    "    if available:\n",
    "        candidates.extend(available)\n",
    "    candidates.extend(['gpt-4o-mini','gpt-4o','gpt-4','gpt-3.5-turbo','groq/llama3-70b-8192'])\n",
    "    seen = set(); ordered = []\n",
    "    for c in candidates:\n",
    "        if c and c not in seen:\n",
    "            ordered.append(c); seen.add(c)\n",
    "    print('Probing candidate models (in order):', ordered[:20])\n",
    "    for cand in ordered:\n",
    "        ok, info = probe_model_candidate(api_key, cand, client_obj=client_obj)\n",
    "        print('Probe', cand, '->', ok)\n",
    "        if ok:\n",
    "            print('Selected working model:', cand)\n",
    "            return cand\n",
    "    print('No working model found by probe.')\n",
    "    return None\n",
    "\n",
    "# Attempt auto-select if API_KEY present\n",
    "API_KEY = os.environ.get('GROQ_API_KEY')\n",
    "if API_KEY:\n",
    "    selected = auto_select_model(API_KEY, client_obj=client)\n",
    "    if selected:\n",
    "        MODEL = selected\n",
    "        print('MODEL auto-selected:', MODEL)\n",
    "    else:\n",
    "        print('MODEL not found automatically. You may need to enable models in the Groq console or use a different key.')\n",
    "else:\n",
    "    print('Skipping model auto-selection; no API key.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation data model and truncation helpers\n",
    "from typing import List, Dict\n",
    "Conversation = List[Dict[str,str]]\n",
    "\n",
    "def append_message(history: Conversation, role: str, content: str):\n",
    "    history.append({'role': role, 'content': content})\n",
    "\n",
    "def truncate_by_turns(history: Conversation, n: int):\n",
    "    if n <= 0: return []\n",
    "    return history[-n:]\n",
    "\n",
    "def truncate_by_chars(history: Conversation, max_chars: int):\n",
    "    new_hist=[]; total=0\n",
    "    for msg in reversed(history):\n",
    "        l = len(msg['content'])\n",
    "        if total + l > max_chars: break\n",
    "        new_hist.insert(0,msg); total += l\n",
    "    return new_hist\n",
    "\n",
    "def truncate_by_words(history: Conversation, max_words: int):\n",
    "    new_hist=[]; total=0\n",
    "    for msg in reversed(history):\n",
    "        w = len(msg['content'].split())\n",
    "        if total + w > max_words: break\n",
    "        new_hist.insert(0,msg); total += w\n",
    "    return new_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization utilities (fallback + LLM wrapper)\n",
    "import re, json\n",
    "\n",
    "def fallback_summarize(history: Conversation, max_sentences=3) -> str:\n",
    "    text = ' '.join([f\"{m['role'].upper()}: {m['content']}\" for m in history])\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return ' '.join(sentences[:max_sentences]).strip()\n",
    "\n",
    "def _parse_chat_response(resp):\n",
    "    if hasattr(resp, 'choices'):\n",
    "        try:\n",
    "            choice = resp.choices[0]\n",
    "            message = getattr(choice, 'message', None)\n",
    "            if message and isinstance(message, dict):\n",
    "                return message.get('content', '')\n",
    "            return message and message.get('content', '') or ''\n",
    "        except Exception:\n",
    "            pass\n",
    "    if isinstance(resp, dict):\n",
    "        choices = resp.get('choices', [])\n",
    "        if choices:\n",
    "            msg = choices[0].get('message', {})\n",
    "            if isinstance(msg, dict):\n",
    "                return msg.get('content', '')\n",
    "    raise ValueError('Unable to parse model response content')\n",
    "\n",
    "def llm_summarize(history: Conversation, model=None, prompt_suffix='Summarize the conversation in 2-3 concise sentences.', max_tokens=200, temperature=0.2):\n",
    "    if not use_llm:\n",
    "        raise RuntimeError('LLM not configured. Provide GROQ_API_KEY.')\n",
    "    if model is None:\n",
    "        model = MODEL\n",
    "    prompt_lines = [f\"{m['role'].upper()}: {m['content']}\" for m in history]\n",
    "    prompt = '\\\\n'.join(prompt_lines) + '\\\\n\\\\n' + prompt_suffix\n",
    "    try:\n",
    "        resp = client.chat.completions.create(model=model, messages=[{'role':'system','content':'You are a concise summarizer.'},{'role':'user','content':prompt}], max_tokens=max_tokens, temperature=temperature)\n",
    "        return _parse_chat_response(resp).strip()\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            resp2 = client.ChatCompletion.create(model=model, messages=[{'role':'system','content':'You are a concise summarizer.'},{'role':'user','content':prompt}], max_tokens=max_tokens, temperature=temperature)\n",
    "            return _parse_chat_response(resp2).strip()\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f'LLM summarization failed: {e1} | {e2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2118e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationManager class\n",
    "from copy import deepcopy\n",
    "\n",
    "class ConversationManager:\n",
    "    def __init__(self, k_periodic:int=3, summarizer=None, fallback_summarizer=None):\n",
    "        self.history: Conversation = []\n",
    "        self.run_count = 0\n",
    "        self.k_periodic = k_periodic\n",
    "        self.summarizer = summarizer or (lambda h: fallback_summarize(h))\n",
    "        self.fallback_summarizer = fallback_summarizer or fallback_summarize\n",
    "\n",
    "    def add_turn(self, role: str, content: str, use_llm_summary: bool=False):\n",
    "        append_message(self.history, role, content)\n",
    "        self.run_count += 1\n",
    "        did_summary = False\n",
    "        if self.k_periodic > 0 and (self.run_count % self.k_periodic) == 0:\n",
    "            try:\n",
    "                if use_llm_summary and use_llm:\n",
    "                    s = self.summarizer(self.history)\n",
    "                else:\n",
    "                    s = self.fallback_summarizer(self.history)\n",
    "            except Exception:\n",
    "                s = self.fallback_summarizer(self.history)\n",
    "            self.history = [{'role':'system','content': f'Summary after {self.run_count} turns: {s}'}]\n",
    "            did_summary = True\n",
    "        return did_summary\n",
    "\n",
    "    def get_history(self):\n",
    "        return deepcopy(self.history)\n",
    "\n",
    "    def truncate_history(self, by_turns=None, by_chars=None, by_words=None):\n",
    "        hist = self.get_history()\n",
    "        if by_turns is not None:\n",
    "            hist = truncate_by_turns(hist, by_turns)\n",
    "        if by_chars is not None:\n",
    "            hist = truncate_by_chars(hist, by_chars)\n",
    "        if by_words is not None:\n",
    "            hist = truncate_by_words(hist, by_words)\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema for contact extraction and function metadata\n",
    "contact_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\":\"string\"},\n",
    "        \"email\": {\"type\":\"string\",\"format\":\"email\"},\n",
    "        \"phone\": {\"type\":\"string\"},\n",
    "        \"location\": {\"type\":\"string\"},\n",
    "        \"age\": {\"type\":\"integer\", \"minimum\": 0}\n",
    "    },\n",
    "    \"required\": [\"name\",\"email\"]\n",
    "}\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"extract_contact_info\",\n",
    "        \"description\": \"Extract contact fields from a chat message\",\n",
    "        \"parameters\": contact_schema\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a894f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse function-calling response, validate, and wrapper for LLM extraction\n",
    "import json\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "def parse_function_call_arguments(resp):\n",
    "    if hasattr(resp, \"choices\"):\n",
    "        choice = resp.choices[0]\n",
    "        message = getattr(choice, \"message\", None) or (choice.get(\"message\") if isinstance(choice, dict) else None)\n",
    "        if message and isinstance(message, dict):\n",
    "            fn = message.get(\"function_call\", {})\n",
    "            args = fn.get(\"arguments\", \"{}\")\n",
    "            if isinstance(args, str):\n",
    "                return json.loads(args)\n",
    "            else:\n",
    "                return args\n",
    "    if isinstance(resp, dict):\n",
    "        choices = resp.get(\"choices\", [])\n",
    "        if choices:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            fn = message.get(\"function_call\", {})\n",
    "            args = fn.get(\"arguments\", \"{}\")\n",
    "            if isinstance(args, str):\n",
    "                return json.loads(args)\n",
    "            else:\n",
    "                return args\n",
    "    raise ValueError(\"Could not parse function_call.arguments from response.\")\n",
    "\n",
    "def validate_extraction(obj, schema=contact_schema):\n",
    "    try:\n",
    "        validate(instance=obj, schema=schema)\n",
    "        return True, None\n",
    "    except ValidationError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def llm_extract_contact(chat_text: str, model=None):\n",
    "    if not use_llm:\n",
    "        raise RuntimeError(\"LLM disabled. Provide GROQ_API_KEY and ensure MODEL is set.\")\n",
    "    if model is None:\n",
    "        model = MODEL\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\":\"system\",\"content\":\"You are a JSON extractor. Return the function_call with arguments that conform exactly to the schema.\"},\n",
    "                      {\"role\":\"user\",\"content\": chat_text}],\n",
    "            functions=functions,\n",
    "            function_call={\"name\": functions[0][\"name\"]},\n",
    "            max_tokens=300,\n",
    "            temperature=0\n",
    "        )\n",
    "        extracted = parse_function_call_arguments(resp)\n",
    "        return extracted, resp\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            resp2 = client.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\":\"system\",\"content\":\"You are a JSON extractor. Return the function_call with arguments that conform exactly to the schema.\"},\n",
    "                          {\"role\":\"user\",\"content\": chat_text}],\n",
    "                functions=functions,\n",
    "                function_call={\"name\": functions[0][\"name\"]},\n",
    "                max_tokens=300,\n",
    "                temperature=0\n",
    "            )\n",
    "            extracted = parse_function_call_arguments(resp2)\n",
    "            return extracted, resp2\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"LLM extraction failed: {e1} | {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5550af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved regex fallback extractor\n",
    "import re\n",
    "def regex_extract_improved(text: str):\n",
    "    out = {}\n",
    "    text = text.strip()\n",
    "    em = re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    if em: out['email'] = em.group(0)\n",
    "    ph = re.search(r'(\\+?\\d[\\d\\-\\s]{6,}\\d)', text)\n",
    "    if ph: out['phone'] = re.sub(r'[\\s\\-]+','', ph.group(0))\n",
    "    age = re.search(r\"(?:I'm|I am|age[:\\s\\-]*)\\s*(\\d{1,3})\", text, flags=re.IGNORECASE)\n",
    "    if age:\n",
    "        try: out['age'] = int(age.group(1))\n",
    "        except: pass\n",
    "    name_patterns = [\n",
    "        r\"(?:my name is|my name's|I am|I'm|this is)\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\",\n",
    "        r\"^([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+){0,2})[,\\.]\",\n",
    "        r\"^Hello[,\\\\s]+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\"\n",
    "    ]\n",
    "    for p in name_patterns:\n",
    "        m = re.search(p, text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            out['name'] = m.group(1).strip()\n",
    "            break\n",
    "    loc = re.search(r'(?:from|in|live in)\\s+([A-Z][a-zA-Z\\s]+)', text, flags=re.IGNORECASE)\n",
    "    if loc: out['location'] = loc.group(1).strip()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration (Task 1 & Task 2)\n",
    "print(\"=== TASK 1: Conversation manager demo ===\")\n",
    "cm = ConversationManager(k_periodic=3)\n",
    "sample_turns = [\n",
    "    (\"user\",\"Hi, I'm Alice. I ordered a laptop last week and it hasn't arrived.\"),\n",
    "    (\"assistant\",\"Sorry to hear that — what's your order id?\"),\n",
    "    (\"user\",\"Order id 12345. I need it by Monday.\"),\n",
    "    (\"assistant\",\"Thanks, I will check and update you.\"),\n",
    "    (\"user\",\"Also change shipping to 123 Baker Street, London.\"),\n",
    "    (\"assistant\",\"Done, I'll confirm.\")\n",
    "]\n",
    "for i,(role,txt) in enumerate(sample_turns,1):\n",
    "    hit = cm.add_turn(role, txt, use_llm_summary=False)\n",
    "    print(f\"Added turn {i} ({role}). Summarized? {hit}\")\n",
    "    print(\"Current history:\", cm.get_history())\n",
    "\n",
    "print(\"\\\\nTruncation example (last 2 turns):\", cm.truncate_history(by_turns=2))\n",
    "\n",
    "print(\"\\\\n=== TASK 2: Extraction demo ===\")\n",
    "samples = [\n",
    "    \"Hi, I'm Soham Banerjee. My email is soham@example.com, phone +91-9876543210. I'm 24 and live in Kolkata.\",\n",
    "    \"Hello, my name is Alice Smith. Contact: alice.smith@example.org. I'm 30, from London.\",\n",
    "    \"Hey, I'm Tom - email tom123@mail.com. Phone 555-1234. From New York.\"\n",
    "]\n",
    "\n",
    "for i,s in enumerate(samples,1):\n",
    "    print(f\"\\\\nSample {i} input:\", s)\n",
    "    if use_llm and MODEL:\n",
    "        try:\n",
    "            extracted, raw = llm_extract_contact(s, model=MODEL)\n",
    "            ok, err = validate_extraction(extracted)\n",
    "            print(\"LLM-extracted:\", extracted)\n",
    "            print(\"Validation OK?\", ok)\n",
    "            if err: print(\"Validation error:\", err)\n",
    "        except Exception as e:\n",
    "            print(\"LLM extraction failed:\", e)\n",
    "            print(\"Falling back to improved regex.\")\n",
    "            extracted = regex_extract_improved(s)\n",
    "            ok, err = validate_extraction(extracted)\n",
    "            print(\"Regex-extracted:\", extracted)\n",
    "            print(\"Validation OK?\", ok, \"Err:\", err)\n",
    "    else:\n",
    "        extracted = regex_extract_improved(s)\n",
    "        ok, err = validate_extraction(extracted)\n",
    "        print(\"Regex-extracted (offline):\", extracted)\n",
    "        print(\"Validation OK?\", ok, \"Err:\", err)\n",
    "\n",
    "print(\"\\\\nDemo finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558540cc",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "- If LLM calls fail with `model_not_found`, run the model discovery/probe cell and set `MODEL` to a reported working id.\n",
    "- Do not commit API keys. Revoke any leaked keys immediately and issue new ones.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
